{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTxOiMLvibHM5bQZFyhS51"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISr8P0x6cC3U"
      },
      "outputs": [],
      "source": [
        "#假设有一个简单的神经网络，\n",
        "#包含一个输入层（2个神经元）\n",
        "#一个隐藏层（3个神经元，使用 ReLU 激活函数）\n",
        "#一个输出层（1个神经元，使用线性激活函数）。\n",
        "\n",
        "#我们用均方误差（Mean Squared Error, MSE）作为损失函数。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "ClD9dBjTcja-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#求relu激活函数\n",
        "def relu(a):\n",
        "  return np.maximum(0, a)"
      ],
      "metadata": {
        "id": "-NszRB2Dd-eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#前向传播，求输出y_pred\n",
        "\n",
        "def forward(W1, W2, b1, b2, X):\n",
        "  Z = np.dot(X, W1) + b1 #shape (1,3)\n",
        "  H = relu(Z) #shape (3,)\n",
        "\n",
        "  y_pred = np.dot(H, W2) + b2\n",
        "  return Z, H, y_pred"
      ],
      "metadata": {
        "id": "P9e_h3a2eVhW"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#求损失函数, loss值\n",
        "def compute_loss(y_pred, y_true):\n",
        "  loss = (y_pred - y_true)**2 / 2\n",
        "  return loss"
      ],
      "metadata": {
        "id": "ocj-M_Azj9dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#反向传播，求梯度函数\n",
        "def backward(y_pred, y_true, H, Z, W2, X):\n",
        "  b2_grad = y_pred - y_true\n",
        "  W2_grad = b2_grad * H\n",
        "\n",
        "  b1_grad = b2_grad * W2 * (Z>0)\n",
        "  W1_grad = np.outer(X, b1_grad)\n",
        "\n",
        "  return W1_grad, W2_grad, b1_grad, b2_grad"
      ],
      "metadata": {
        "id": "u1ao42KDk84P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#更新参数，weight,bias\n",
        "def update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad, lr):\n",
        "  W1 -= lr * W1_grad\n",
        "  W2 -= lr * W2_grad\n",
        "  b1 -= lr * b1_grad\n",
        "  b2 -= lr * b2_grad\n",
        "  return W1, W2, b1, b2"
      ],
      "metadata": {
        "id": "57cwWCulpADo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([1.0, 2.0]) #ndarray(2,)（一个样本，2个特征）\n",
        "\n",
        "y_true = 3.0\n",
        "\n",
        "W1 = np.array([[0.5, 0.1, -0.3],\n",
        "        [0.2, 0.4, 0.6]]) #ndarray(2,3)\n",
        "\n",
        "b1 = np.array([0.1, 0.2, 0.3]) #ndarray(3,)\n",
        "\n",
        "W2 = np.array([0.3, 0.7, -0.5]) #ndarray(3,) （形状为 3×1）\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "b2 = 0.1\n",
        "\n",
        "epochs = 30\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  Z, H, y_pred = forward(W1, W2, b1, b2, X)\n",
        "  loss = compute_loss(y_pred, y_true)\n",
        "  W1_grad, W2_grad, b1_grad, b2_grad = backward(y_pred, y_true, H, Z, W2, X)\n",
        "  W1, W2, b1, b2 = update_params(W1, W2, b1, b2, W1_grad, W2_grad, b1_grad, b2_grad, lr)\n",
        "\n",
        "  print(\"Epoch:\", epoch + 1,\n",
        "    \"loss:\", np.round(loss, 4),\n",
        "    \"W1_grad\", np.round(W1_grad.flatten(), 2),\n",
        "    \"W2_grad\", np.round(W2_grad.flatten(), 2),\n",
        "    \"b1_grad\", np.round(b1_grad.flatten(), 2),\n",
        "    \"b2_grad\", np.round(b2_grad.flatten(), 2)\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s5JDu_VcnlW",
        "outputId": "741ac9db-b6ab-4e56-c175-75baba1d9ce1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 loss: 2.9524 W1_grad [-0.73 -1.7   1.21 -1.46 -3.4   2.43] W2_grad [-2.43 -2.67 -2.92] b1_grad [-0.73 -1.7   1.21] b2_grad [-2.43]\n",
            "Epoch: 2 loss: 2.4075 W1_grad [-0.71 -1.59  1.03 -1.42 -3.19  2.07] W2_grad [-2.29 -2.64 -2.47] b1_grad [-0.71 -1.59  1.03] b2_grad [-2.19]\n",
            "Epoch: 3 loss: 1.949 W1_grad [-0.69 -1.49  0.88 -1.37 -2.97  1.76] W2_grad [-2.15 -2.56 -2.1 ] b1_grad [-0.69 -1.49  0.88] b2_grad [-1.97]\n",
            "Epoch: 4 loss: 1.5639 W1_grad [-0.65 -1.38  0.75 -1.3  -2.75  1.5 ] W2_grad [-1.99 -2.45 -1.79] b1_grad [-0.65 -1.38  0.75] b2_grad [-1.77]\n",
            "Epoch: 5 loss: 1.2424 W1_grad [-0.61 -1.27  0.64 -1.23 -2.53  1.28] W2_grad [-1.84 -2.32 -1.52] b1_grad [-0.61 -1.27  0.64] b2_grad [-1.58]\n",
            "Epoch: 6 loss: 0.9764 W1_grad [-0.57 -1.15  0.55 -1.14 -2.31  1.1 ] W2_grad [-1.68 -2.16 -1.3 ] b1_grad [-0.57 -1.15  0.55] b2_grad [-1.4]\n",
            "Epoch: 7 loss: 0.7591 W1_grad [-0.52 -1.04  0.47 -1.04 -2.09  0.93] W2_grad [-1.52 -1.99 -1.1 ] b1_grad [-0.52 -1.04  0.47] b2_grad [-1.23]\n",
            "Epoch: 8 loss: 0.5837 W1_grad [-0.47 -0.94  0.4  -0.95 -1.88  0.8 ] W2_grad [-1.37 -1.81 -0.94] b1_grad [-0.47 -0.94  0.4 ] b2_grad [-1.08]\n",
            "Epoch: 9 loss: 0.4441 W1_grad [-0.43 -0.84  0.34 -0.85 -1.67  0.68] W2_grad [-1.22 -1.63 -0.8 ] b1_grad [-0.43 -0.84  0.34] b2_grad [-0.94]\n",
            "Epoch: 10 loss: 0.3345 W1_grad [-0.38 -0.74  0.29 -0.76 -1.48  0.57] W2_grad [-1.08 -1.46 -0.67] b1_grad [-0.38 -0.74  0.29] b2_grad [-0.82]\n",
            "Epoch: 11 loss: 0.2495 W1_grad [-0.34 -0.65  0.24 -0.67 -1.3   0.49] W2_grad [-0.95 -1.29 -0.57] b1_grad [-0.34 -0.65  0.24] b2_grad [-0.71]\n",
            "Epoch: 12 loss: 0.1845 W1_grad [-0.29 -0.56  0.21 -0.59 -1.13  0.41] W2_grad [-0.83 -1.13 -0.48] b1_grad [-0.29 -0.56  0.21] b2_grad [-0.61]\n",
            "Epoch: 13 loss: 0.1354 W1_grad [-0.26 -0.49  0.17 -0.51 -0.98  0.35] W2_grad [-0.72 -0.99 -0.41] b1_grad [-0.26 -0.49  0.17] b2_grad [-0.52]\n",
            "Epoch: 14 loss: 0.0986 W1_grad [-0.22 -0.42  0.15 -0.44 -0.84  0.29] W2_grad [-0.62 -0.86 -0.34] b1_grad [-0.22 -0.42  0.15] b2_grad [-0.44]\n",
            "Epoch: 15 loss: 0.0714 W1_grad [-0.19 -0.36  0.12 -0.38 -0.73  0.25] W2_grad [-0.53 -0.74 -0.29] b1_grad [-0.19 -0.36  0.12] b2_grad [-0.38]\n",
            "Epoch: 16 loss: 0.0514 W1_grad [-0.16 -0.31  0.1  -0.33 -0.62  0.21] W2_grad [-0.46 -0.63 -0.24] b1_grad [-0.16 -0.31  0.1 ] b2_grad [-0.32]\n",
            "Epoch: 17 loss: 0.0368 W1_grad [-0.14 -0.26  0.09 -0.28 -0.53  0.17] W2_grad [-0.39 -0.54 -0.2 ] b1_grad [-0.14 -0.26  0.09] b2_grad [-0.27]\n",
            "Epoch: 18 loss: 0.0263 W1_grad [-0.12 -0.22  0.07 -0.24 -0.45  0.15] W2_grad [-0.33 -0.46 -0.17] b1_grad [-0.12 -0.22  0.07] b2_grad [-0.23]\n",
            "Epoch: 19 loss: 0.0187 W1_grad [-0.1  -0.19  0.06 -0.2  -0.38  0.12] W2_grad [-0.28 -0.39 -0.14] b1_grad [-0.1  -0.19  0.06] b2_grad [-0.19]\n",
            "Epoch: 20 loss: 0.0132 W1_grad [-0.09 -0.16  0.05 -0.17 -0.32  0.1 ] W2_grad [-0.24 -0.33 -0.12] b1_grad [-0.09 -0.16  0.05] b2_grad [-0.16]\n",
            "Epoch: 21 loss: 0.0093 W1_grad [-0.07 -0.14  0.04 -0.14 -0.27  0.09] W2_grad [-0.2  -0.28 -0.1 ] b1_grad [-0.07 -0.14  0.04] b2_grad [-0.14]\n",
            "Epoch: 22 loss: 0.0066 W1_grad [-0.06 -0.11  0.04 -0.12 -0.23  0.07] W2_grad [-0.17 -0.24 -0.08] b1_grad [-0.06 -0.11  0.04] b2_grad [-0.11]\n",
            "Epoch: 23 loss: 0.0046 W1_grad [-0.05 -0.1   0.03 -0.1  -0.19  0.06] W2_grad [-0.14 -0.2  -0.07] b1_grad [-0.05 -0.1   0.03] b2_grad [-0.1]\n",
            "Epoch: 24 loss: 0.0033 W1_grad [-0.04 -0.08  0.03 -0.09 -0.16  0.05] W2_grad [-0.12 -0.17 -0.06] b1_grad [-0.04 -0.08  0.03] b2_grad [-0.08]\n",
            "Epoch: 25 loss: 0.0023 W1_grad [-0.04 -0.07  0.02 -0.07 -0.14  0.04] W2_grad [-0.1  -0.14 -0.05] b1_grad [-0.04 -0.07  0.02] b2_grad [-0.07]\n",
            "Epoch: 26 loss: 0.0016 W1_grad [-0.03 -0.06  0.02 -0.06 -0.11  0.04] W2_grad [-0.08 -0.12 -0.04] b1_grad [-0.03 -0.06  0.02] b2_grad [-0.06]\n",
            "Epoch: 27 loss: 0.0011 W1_grad [-0.03 -0.05  0.01 -0.05 -0.09  0.03] W2_grad [-0.07 -0.1  -0.03] b1_grad [-0.03 -0.05  0.01] b2_grad [-0.05]\n",
            "Epoch: 28 loss: 0.0008 W1_grad [-0.02 -0.04  0.01 -0.04 -0.08  0.02] W2_grad [-0.06 -0.08 -0.03] b1_grad [-0.02 -0.04  0.01] b2_grad [-0.04]\n",
            "Epoch: 29 loss: 0.0005 W1_grad [-0.02 -0.03  0.01 -0.04 -0.07  0.02] W2_grad [-0.05 -0.07 -0.02] b1_grad [-0.02 -0.03  0.01] b2_grad [-0.03]\n",
            "Epoch: 30 loss: 0.0004 W1_grad [-0.01 -0.03  0.01 -0.03 -0.06  0.02] W2_grad [-0.04 -0.06 -0.02] b1_grad [-0.01 -0.03  0.01] b2_grad [-0.03]\n"
          ]
        }
      ]
    }
  ]
}